{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGhjnrXUiKTA"
      },
      "source": [
        "# Assignment 3: Introduction to Deep Learning [10 pts]\n",
        "\n",
        "CS-GY 9223: Machine Listening\n",
        "\n",
        "Below you will find a mix of coding questions and writing questions to familiarize you with the fundamentals of signal processing in Python.\n",
        "\n",
        "**Read through the text, code, and comments carefully and fill-in the blanks accordingly. Written questions will be denoted with ⚠️ , and code questions will be explained in code comments, both with \"TODO\" markers.**\n",
        "\n",
        "\n",
        "The assignment will be 10 points total, with 1 bonus point\n",
        "\n",
        "For this assignment, I recommend **using Google Colab** for performance boosts using GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZDpKa3NlUR9"
      },
      "source": [
        "# Part 0: PyTorch Fundamentals [0 pts]\n",
        "⚠️ This section is not worth any points! If you are new to PyTorch and training neural networks, I highly recommend walking thorugh these steps. If not, I still recommend glancing at the dataloader and model training script structure, as it will be helpful later on - but no need to spend much time on it.\n",
        "\n",
        "We'll be using PyTorch in this course for deep learning! In this section we will walk through some basics of Torch, setting up a dataset/\"dataloader\", and a general template for training a model. For more tutorials on PyTorch basics, check out the Torch website [here](https://pytorch.org/tutorials/beginner/basics/intro.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Bt521c9Z-Qo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "ts8n7CKw-RdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary"
      ],
      "metadata": {
        "id": "xaMuZOdB-STL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL6wfK37mcJv"
      },
      "source": [
        "### PyTorch Tensors\n",
        "[**Tensors**](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html) are the core data type you will work with in PyTorch. PyTorch tensors and NumPy arrays are both multi-dimensional data structures used for numerical computations, but tensors are optimized for GPU acceleration and support automatic differentiation (autograd), making them ideal for deep learning. NumPy arrays are primarily CPU-based and lack built-in support for gradients. Check out some examples of tensor manipulations below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joyKP4AdlVCK"
      },
      "outputs": [],
      "source": [
        "# Creating a Tensor (like a NumPy array but with GPU support)\n",
        "x = torch.tensor([[1.0, 2.0], [3.0, 4.0]])  # 2x2 matrix\n",
        "print(x)\n",
        "\n",
        "# Creating a Random Tensor\n",
        "rand_tensor = torch.rand(3, 3)  # 3x3 matrix with random values\n",
        "print(rand_tensor)\n",
        "\n",
        "# Basic Tensor Operations\n",
        "y = x + 2  # Element-wise addition\n",
        "z = x * y  # Element-wise multiplication\n",
        "print(\"Added Tensor:\\n\", y)\n",
        "print(\"Multiplied Tensor:\\n\", z)\n",
        "\n",
        "# Moving Tensors to GPU (if available)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # You may see this line of code at the top of all\n",
        "x_gpu = x.to(device)  # Moves tensor to GPU\n",
        "print(\"Tensor on device:\", x_gpu.device)\n",
        "\n",
        "# Reshaping a Tensor\n",
        "reshaped = x.view(4, 1)  # Reshape to 4x1\n",
        "print(\"Reshaped Tensor:\\n\", reshaped)\n",
        "\n",
        "# Converting Between NumPy and Torch\n",
        "numpy_array = x.numpy()  # Convert to NumPy\n",
        "torch_tensor = torch.from_numpy(numpy_array)  # Convert back to PyTorch tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cToYl1AHpZmH"
      },
      "source": [
        "### Datasets and dataloaders\n",
        "In PyTorch, we often work with **iterable datasets**, which are wrapped in a **dataloader** class. This provides a nice way to iterate through our data as we train our model. See the [docs](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for more info here.\n",
        "\n",
        "Below I've provided a *very* simple template dataset and dataloader to give you a framework to build off of as we get into the audio datasets below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdWbGdp1pdzL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Our simple dataset class\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data  # (ex: numpy array of (n_samples, n_features), list of filepaths etc.)\n",
        "        self.labels = labels  # (ex: list of class labels)\n",
        "\n",
        "        print(f'Number of data samples: {len(self.data)}')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)  # Return dataset size, this is just a formality\n",
        "\n",
        "    # Get item is the core method that retrieves one sample (and a label, optionally)\n",
        "    def __getitem__(self, idx):\n",
        "        # idx is the index into your full dataset (e.g. sample at index 2 from your dataset)\n",
        "        return self.data[idx], self.labels[idx]  # Return sample and label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFxpyR8Jpg2q"
      },
      "outputs": [],
      "source": [
        "# Dummy data\n",
        "data = torch.randn(100, 10)  # 100 samples, each with 10 features\n",
        "labels = torch.randint(0, 2, (100,))  # 100 binary labels\n",
        "\n",
        "# Create dataset and DataLoader\n",
        "dataset = SimpleDataset(data, labels)\n",
        "\n",
        "# Dataloader is an iterable wrapper class for dataset\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Iterate over DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(inputs.shape, targets.shape)\n",
        "    break  # Stop after first batch for demo\n",
        "\n",
        "# Note that if you are using different train/val/test splits, usually\n",
        "# you create separate datasets+dataloaders for each of the splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVLO--J-3YDP"
      },
      "source": [
        "### Model definition\n",
        "Below we define a simple single-layer model class in PyTorch. Read through the comments to understandn the `__init__` and `forward()` functionalities. Note that the weights and biases of your model layers are \"under the hood\" below - we won't be explicitly defining them here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy9mnWEU3iJ4"
      },
      "outputs": [],
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_channels, n_classes):\n",
        "        super(SimpleModel, self).__init__() # formality\n",
        "        # Here we define the model architecture\n",
        "        # This *doesn't* tell us how data flows through the model, just the architecture\n",
        "\n",
        "        # Define one single linear layer\n",
        "        # This has to accept 2-dim input and return n_classes output\n",
        "        # Input should be (batch_size, input_channels)\n",
        "        # Linear layer operates on channel dimension only\n",
        "        self.fc = nn.Linear(input_channels, n_classes)\n",
        "\n",
        "    # The forward pass is the core method in a model class\n",
        "    # This determines how data x flows through the network\n",
        "    def forward(self, x):\n",
        "        output = self.fc(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY8leQwC4fwG"
      },
      "outputs": [],
      "source": [
        "# Simple function to get the model config and number of parameters\n",
        "def print_model(model):\n",
        "    # Print model's state_dict\n",
        "    print(\"Model's state dictionary (stored weights):\")\n",
        "    for param_tensor in model.state_dict():\n",
        "        print(\"  \", param_tensor, \"\\t\", tuple(model.state_dict()[param_tensor].size()))\n",
        "\n",
        "    # Print the number of parameters in the model\n",
        "    parameter_count =  sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(\"In total, this network has \", parameter_count, \" trainable parameters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQE0jtVs41Dq"
      },
      "outputs": [],
      "source": [
        "print_model(SimpleModel(input_channels=2, n_classes=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLO2pwNZ5CUG"
      },
      "outputs": [],
      "source": [
        "# Let's test out passing some dummy data through the model\n",
        "# This is only a *forward* pass through the model e.g. \"inference\" - not training or doing any back propogation\n",
        "model = SimpleModel(input_channels=2, n_classes=5)\n",
        "sample_data = torch.randn(10,2)\n",
        "output = model(sample_data)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66WFocWH6GLI"
      },
      "source": [
        "### Backpropogation and optimization in pytorch\n",
        "In neural networks, the model will learn to map inputs through intermediate (\"hidden\") representations of varying dimensions. This complexity helps our model learn more complex mapping functions, but adds a level of difficulty in figuring out how to update the weights of all of our parameters of the model (e.g. with gradient descent!).\n",
        "\n",
        "\n",
        "To update our network given a target and predicted output, we will compute the loss (e.g. mean-squared error) using a differentiable loss function, and then compute the gradient of the loss function with respect to each model weight, and performing a small update in the opposite direction of the gradient. The computation of these gradients is called **backpropagation**, and allows us to systematically train large and complex neural networks.\n",
        "\n",
        "Luckily, PyTorch provides automatic differentiation (e.g. autograd), which provides a built-in gradient computation for us! Let's play with a few aspects of gradients in PyTorch before incorporating the model component.\n",
        "\n",
        "[Source credit for this section](https://github.com/interactiveaudiolab/course-deep-learning/blob/main/notebooks/notebook_2_nn.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBzgpA6CSV3x"
      },
      "outputs": [],
      "source": [
        "# Note that below, we aren't actually doing any \"model training\"\n",
        "# This shows how we would do manual optimization - without using built-in optimization yet.\n",
        "\n",
        "# Sample tensor\n",
        "x = torch.ones(5)\n",
        "print(f\"Creating a tensor of type {type(x)} with shape {x.shape}\")\n",
        "print(f\"Starting x: {x}\")\n",
        "\n",
        "# During backpropagation, gradients will only be computed for tensors with the\n",
        "# `requires_grad` attribute set to True. We can set this manually if need be\n",
        "print(f\"Does our tensor require gradient computation? {x.requires_grad}\")\n",
        "x.requires_grad = True\n",
        "print(f\"Does our tensor require gradient computation? {x.requires_grad}\")\n",
        "\n",
        "# To perform backpropagation, we need to complete a \"forward pass\" in which\n",
        "# computations are performed on Tensor objects to compute a scalar loss value\n",
        "# This is just a dummy scalar loss function - in practice this will\n",
        "loss = 10 - x.sum()\n",
        "print(f\"Starting `loss` value: {loss}\")\n",
        "print(f\"Gradients of x: {x.grad}\") # no gradients yet\n",
        "print(f\"Loss function requires_grad?: {loss.requires_grad}\")\n",
        "\n",
        "# PyTorch will compute all required gradients for tensors involved in the\n",
        "# computation of a scalar loss value once we call `.backward()`\n",
        "loss.backward()\n",
        "print(f\"Gradients of x: {x.grad}\")\n",
        "\n",
        "# We can manually update our `weights` in the opposite direction of this gradient\n",
        "# to reduce our loss value!\n",
        "x = x - x.grad\n",
        "print(f\"Updated x: {x.data}\")\n",
        "loss = 10 - x.sum()\n",
        "print(f\"Updated `loss` value: {loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOnq9wyGVYfT"
      },
      "source": [
        "In the above example we computed differentiable a scalar loss, used backpropagation to compute the gradients of the loss with respect to our \"weights,\" and performed a gradient-based update on our weights to reduce the loss. Rather than managing the weight-update process by hand, we can defer to a **built-in optimizer** object that automatically adjusts weights based on stored gradients and standard hyperparameters (e.g. learning rate). When training neural networks with large numbers of parameters, this becomes much simpler than manually updating each weight. [credit](https://github.com/interactiveaudiolab/course-deep-learning/blob/main/notebooks/notebook_2_nn.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdX4IeDTVpZS"
      },
      "outputs": [],
      "source": [
        "# Repeat our simple optimization, this time using the optimizer.\n",
        "x = torch.ones(5).requires_grad_(True)\n",
        "print(f\"Starting x: {x}\")\n",
        "\n",
        "# Create an optimizer object and pass it an Iterable containing our \"weights\".\n",
        "# Here, SGD is the torch stochastic gradient descent optimizer.\n",
        "# It has been handed our tensor x as something to optimize and the learning rate\n",
        "# (lr) is set to 1, which determines the step size for making changes to x.\n",
        "# Note that this example learning rate is very high! in practice we usuallyuse something like 0.1 or 0.01.\n",
        "opt = torch.optim.SGD([x], lr = 1.0)\n",
        "\n",
        "# Compute loss and perform backpropagation.\n",
        "loss = 10 - x.sum()\n",
        "loss.backward()\n",
        "\n",
        "# perform an automatic optimization step, i.e. a gradient-based update of our weights\n",
        "opt.step()\n",
        "\n",
        "print(f\"Updated x: {x}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLpWsIPq3cNJ"
      },
      "source": [
        "### Simple training pipeline\n",
        "Finally, let's put all of the pieces together and write a simple training script for a dummy classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93xTFxbc6Ogv"
      },
      "outputs": [],
      "source": [
        "# Data setup\n",
        "train_data = torch.randn(100, 10, dtype=torch.float32)  # 100 samples, each with 10 features\n",
        "train_labels = torch.randint(0, 5, (100,))  # 100 labels between 0-4\n",
        "\n",
        "# Create dataset and DataLoader\n",
        "train_dataset = SimpleDataset(train_data, train_labels)\n",
        "\n",
        "# Dataloader is an iterable wrapper class for dataset\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Also create the validation dataloader\n",
        "val_data = torch.randn(20, 10, dtype=torch.float32)\n",
        "val_labels = torch.randint(0, 5, (20,))\n",
        "val_dataset = SimpleDataset(val_data, val_labels)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Model setup\n",
        "model = SimpleModel(input_channels=10, n_classes=5)\n",
        "\n",
        "# Optimizer setup, on all of our model parameters\n",
        "opt = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "\n",
        "# This tells your model that you are in training mode and not testing mode\n",
        "# For our simple case this doesn't do much, but more complex layers such as Dropout\n",
        "# behave differently in training vs. evaluation mode\n",
        "model.train()\n",
        "\n",
        "\n",
        "# TRAINING LOOP\n",
        "\n",
        "# Loop through the entire [training] data each \"epoch\"\n",
        "# So you will go through every \"batch\" of data inside this epoch\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    for batch in train_dataloader:\n",
        "        inputs, targets = batch\n",
        "        # Set the gradients to 0 before running the network on the data each iteration, so that\n",
        "        # loss gradients can be computed correctly during backpropagation\n",
        "        opt.zero_grad()\n",
        "        # print(f\"Model input shape (batch): {inputs.shape}\")\n",
        "\n",
        "        # Get the output of the network on the data\n",
        "        # Note that these are probabilities, *not* class labels!\n",
        "        output = model(inputs)\n",
        "        # print(f\"Model output shape (batch): {output.shape}\")\n",
        "\n",
        "        # Measure the \"loss\" using mean squared error\n",
        "        loss = loss_fn(output, targets)\n",
        "\n",
        "        # This calculates the gradients, performing backpropagation to propagate\n",
        "        # errors backward through the network's weights\n",
        "        loss.backward()\n",
        "\n",
        "        # This updates the network weights based on the freshly-computed gradient\n",
        "        # now stored alongside each weight\n",
        "        opt.step()\n",
        "\n",
        "        # Accumulate the total loss per epoch\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Training Epoch {epoch+1}/{epochs}, Train Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    # VALIDATION LOOP\n",
        "    # The validation loop\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for batch in val_dataloader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            # Compute validation accuracy\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            correct_val += (predicted == targets).sum().item()\n",
        "            total_val += targets.size(0)\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "    val_accuracy = correct_val / total_val\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Validation Epoch {epoch+1}/{epochs}, Val: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrj45MVocUAA"
      },
      "source": [
        "### Model evaluation\n",
        "Now that we've \"trained\" our dummy model, let's walk set up the evaluation script. Note that typically you will have a training and validation loop, which run in sequence for N epochs, and once you have the final trained model, then you will do the final evaluation on the test set separately - similar to what we did with cross validation in assignment 2. But for this demo, we'll just do the training above and then use that trained model for a fake test loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLlHadPPb8NN"
      },
      "outputs": [],
      "source": [
        "# Create dataset and DataLoader\n",
        "test_data = torch.randn(20, 10, dtype=torch.float32)\n",
        "test_labels = torch.randint(0, 5, (20,))\n",
        "test_dataset = SimpleDataset(test_data, test_labels)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Switch model to eval mode (IMPORTANT!)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "num_correct = 0\n",
        "\n",
        "# Explicitly stop gradient computation here\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        inputs, targets = batch\n",
        "        output = model(inputs)\n",
        "\n",
        "        # Here we don't care as much about the loss (though you can still compute it)\n",
        "        # Convert the output logits to class probabilities\n",
        "        # Why didn't we do this argmax in training? CrossEntropy loss under the hood uses softmax :)\n",
        "        predictions = torch.argmax(output, dim=-1)\n",
        "        num_correct += (predictions == targets).sum().item()  # Count correct predictions\n",
        "\n",
        "print(f\"Test Accuracy: {num_correct / len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpD3hCIEeZRm"
      },
      "source": [
        "⚔️⚔️⚔️ Awesome! You are now equipped with the basic tools needed to start training real deep learning models! ⚔️⚔️⚔️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg7P9hsF6O9D"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuK7dA9Vi4tW"
      },
      "source": [
        "# Part 1: Sound event classification using neural networks [6 pts]\n",
        "Before, we worked with linear classifiers such as linear SVM and logistic regression. However, with real-world, complex data, the data is often not linearly separable (e.g. the class boundaries of our features cannot be separated by a straight line or hyperplane). Additionally, logistic regression and SVMs rely on manually crafted features (e.g. MFCCs), whereas neural networks are effectively automatic feature extractors - learning features automatically in a way that is most helpful for the target task.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "For this part of the assignment we will be working with the same dataset as in Assignment 2 ([**ESC-50**](https://github.com/karolpiczak/ESC-50)).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Data Downloading\n",
        "1. Download the zip of the ESC-50 dataset from Google drive here: https://drive.google.com/file/d/1o2Zt2UYcUtyJtYFcgCl6yuJu-sHMJEN_/view.\n",
        "2. Upload it to *your* Google Drive, so you can access it here via mounting.\n",
        "3. Unzip inside the Colab virtual machine (MUCH faster than loading individual files from drive)\n",
        "    - ex: `!unzip \"/content/drive/MyDrive/ESC-50-master.zip\" -d \"/content\"`\n",
        "4. Use the code below to mount your Google drive to this Colab notebook.\n",
        "5. Now you can access the data here - but note you'll have to unzip agin if the Colab notebook kernel restarts/goes down.\n",
        "\n"
      ],
      "metadata": {
        "id": "fRpaiKbFpL--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/MyDrive/ESC-50-master.zip\" -d \"/content\""
      ],
      "metadata": {
        "id": "WADLCkrAH5pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fV16qo2GH1FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRTo6NEBqCb2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxODLJnPm0rB"
      },
      "source": [
        "### 🔎 But first: check out the data 🔎\n",
        "Even though you're familiar from the last assignment, this is always a good step to make sure the input and target for your model are exactly what you are expecting.\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "1. Load an audio file from ESC-50 (in the `audio` directory).\n",
        "2. Plot the waveform\n",
        "3. Plot the log mel spectrogram\n",
        "4. Play the audio\n",
        "5. Load the full metadata csv (in `esc50.csv`), locate the sample you're examining, and confirm that your label matches what you're expecting, and what you see in the spectrogram."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : the 5 exploratory data points above\n"
      ],
      "metadata": {
        "id": "1Y-6EfmvH9_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25MsX4lNm2q5"
      },
      "source": [
        "### ⌛ Design your ESC50 dataloader ⌛"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tut2CI54iKqd"
      },
      "outputs": [],
      "source": [
        "# TODO : design your ESC50 dataloader, filling in the blanks and building off of the template in Part 0\n",
        "class ESC50Dataset(Dataset):\n",
        "    def __init__(self, data_dir, data_split, spec_type=\"log_mel\", sr=44100, n_fft=1024, hop_length=512, n_mels=128):\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        dataframe = pd.read_csv(f'{self.data_dir}/meta/esc50.csv')\n",
        "\n",
        "        # TODO : Based on data_split arg, filter the dataframe by fold\n",
        "        # Use folds 1,2,3 for train, 4 for val, 5 for test\n",
        "        # You will initialize a separate instance of this class for each split.\n",
        "        filtered_df = None # use the data_split_arg\n",
        "\n",
        "        # TODO : get lists of audio paths and their associated labels\n",
        "        self.audio_paths = None\n",
        "        self.labels = None\n",
        "\n",
        "        # All of the the spectrogram parameters\n",
        "        self.spec_type = spec_type\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.n_mels = n_mels\n",
        "        self.sr = sr\n",
        "\n",
        "        print(f'Number of files in {data_split}: {len(self.audio_paths)}')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # TODO :\n",
        "        # (1) load your wavefrom at the specified sample rate\n",
        "        # (2) get the associated label\n",
        "        # (3) given a spectrogram argument self.spec_type, compute the appropriate feature with librosa\n",
        "        #     (3 continued) spec_type arg can be [lin_pwr, log, log_mel]\n",
        "        # (4) uncomment the spectrogram normalization line\n",
        "        # (5) return the spectrogram and sound class label\n",
        "\n",
        "        label = None\n",
        "        spectrogram = None # use the params from __init__\n",
        "\n",
        "        # TODO : uncomment this line for spectrogram standardization\n",
        "        # You could also try per-sample min-max normalization like we did in assign. 2!\n",
        "        # spectrogram = (spectrogram - spectrogram.mean()) / (spectrogram.std() + 1e-6)\n",
        "\n",
        "        return spectrogram, label"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔎 Test out your dataloader before we get into the model code 🔎"
      ],
      "metadata": {
        "id": "xTRW3nXgJpUW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttnGFnMvzHQp"
      },
      "outputs": [],
      "source": [
        "# TODO : Instantiate your ESC50Dataset class and dataloader\n",
        "dataset = None\n",
        "dataloader = None\n",
        "\n",
        "# TODO : Iterate over the dataloader and print the shapes of your spectrogram and label batch for one batch\n",
        "print('my dataloader shapes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnqZ6Dk23C-U"
      },
      "source": [
        "### 🤖 Define your models 🤖\n",
        "You will now design two models for sound event classification.\n",
        "\n",
        "For both models:\n",
        "- **Model input**: a log-mel spectrogram and it's associated class label (index). Shape: `(batch, 128, 431)`\n",
        "- **Model output**: a vector of probabilities over the number of classes, that we use to get the predicted sound class. Shape:  `(batch, 50)`\n",
        "\n",
        "**TODO:** define two model classes, templated in the code below. Here are some tips for each:\n",
        "\n",
        "1. **Multi-Layer Perceptron (MLP)**\n",
        "    - Define a 2-layer MLP\n",
        "    - MLP's cannot operate on 2D data, so you will need to flatten the spectrogram input as the first step\n",
        "    - Use a `relu` activation between your linear layers\n",
        "\n",
        "2. **Conv-1D**\n",
        "    - Define a model with 2 x Conv1D layers followed by a pooling layer, and a final linear layer mapping to the number of output classes. Ideally these convolutional layers will reduce the dimensionality of your spectrogram incrementally.\n",
        "    - Your convolutional layers should\n",
        "    - Use `relu` activations after each conv layer\n",
        "    - If you are having difficulty with overfitting (e.g. your model is learning your training set well but not generalizing to your validation set), try using [BatchNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) or [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) to help with generalization and to stabilize training.\n",
        "    \n",
        "\n",
        "***How do I know if my model is doing well?:***\n",
        "- Recall that the random baseline for 50-way classification is 2% accuracy - if you are doing better than this, your model is learning!\n",
        "- Don't worry if you are not getting super high accuracy still! You will not be graded mainly on your final model performance - more on your architecture design decisions and analysis.\n",
        "- ⚠️However! If your training and validation accuracies are very low (i.e. around random chance) - there is probably something wrong with your combination of model architecture and training setup, and that will be penalized.\n",
        "- As a reference, my simple baselines for this exercise scored around 30% validation accuracy from the MLP and 40% accuracy from the Conv1D. But again, there are many design decisions at play here and if you're not getting there (or are far exceeding this), that's ok too!\n",
        "- In my experiments I saw a lot of overfitting (e.g. the training accuracy was much higher than the validation accuracy). As long as the validation loss isn't *increasing*, that's not necessarily the worst thing - but ideally we want the loss and accuracy of training and validation to be close. The dataset is small and so are these architectures, so don't worry if this is happening to you too! If you're curious to learn more about this, do some reasearch on strategies to ameliorate this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15VPOAR6aYGc"
      },
      "outputs": [],
      "source": [
        "# TODO : complete the MLP classifier class\n",
        "class MLPClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP tips:\n",
        "        - Define a 2-layer MLP\n",
        "        - MLP's cannot operate on 2D data, so you will need to flatten the spectrogram input as the first step\n",
        "        - Use a relu activation between your linear layers\n",
        "\n",
        "    Input Shape: (batch, 128, 431)\n",
        "    Output Shape: (batch, 50)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=(128, 431), hidden_dim=256, num_classes=50):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        # TODO: define your model layers here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO : define how the data will flow through the layers here\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb7EVJbmk3uc"
      },
      "outputs": [],
      "source": [
        "# TODO : complete the Conv1D classifier class\n",
        "class Conv1DClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Conv-1D\n",
        "        - Define a model with 2 x Conv1D layers followed by a pooling layer, and a final linear layer mapping to the number of output classes.\n",
        "        - Ideally these convolutional layers will reduce the dimensionality of your spectrogram incrementally.\n",
        "        - Your convolutional layers should use relu activations after each conv layer\n",
        "        - Try using BatchNorm1d or Dropout if you are seeing overfitting\n",
        "\n",
        "    Input Shape: (batch, 128, 431)\n",
        "    Output Shape: (batch, 50)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape=(128, 431), hidden_dim=256, num_classes=50):\n",
        "        super(Conv1DClassifier, self).__init__()\n",
        "        # TODO: define your model layers here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO : define how the data will flow through the layers here\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-HAwtmSrFjZ"
      },
      "source": [
        "#### 🔎 Check out your models 🔎\n",
        "Torch info provides a nice summary feature that shows us the number of model parameters and shapes at each layer. Let's check this out before getting into training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary"
      ],
      "metadata": {
        "id": "2CIx_IYjTpcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt0CKdo7rIlX"
      },
      "outputs": [],
      "source": [
        "# TODO : run this cell and make sure your input, layer, and output shapes make sense\n",
        "mlp_model = MLPClassifier(input_shape=(128,431), hidden_dim=256, num_classes=50)\n",
        "summary(mlp_model, input_size=(4,1,128,431))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzlPE25arc3H"
      },
      "outputs": [],
      "source": [
        "# TODO : run this cell and make sure your input, layer, and output shapes make sense\n",
        "cnn1_model = Conv1DClassifier(input_shape=(128,431), num_classes=50)\n",
        "summary(cnn1_model, input_size=(4,128,431))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "q0dIHauOlUxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⚠️ **TODO Written Questions:** ⚠️\n",
        "1. Explain your architecture choices (e.g. hidden dimension size, activations, any additional choices made) for your MLP and Conv1D classifiers above.\n",
        "2. Instantiate a version of your two models where they have the same hidden dimension. Get the number of parameters using the summary method above. How does the number of parameters differ between the two?\n",
        "3. Explain (in theory) the pros and cons of MLPs vs. convolutional 1D methods. We will test your theories out next (also it's okay if your theory doesn't prove to be true! there are many different factors at play here and not one right answer)."
      ],
      "metadata": {
        "id": "2Mxb_ZCwUWwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "k-bxw8zulVn4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoX7GYh3oFUC"
      },
      "source": [
        "### ✏️ Design your training script ✏️\n",
        "TODO: Let's bring the pieces all together and get these models training. Fill in the template below for your model training. Include a training and validation loop per epoch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Complete the method below\n",
        "# You can use the same method for training the MLP and Conv1D here, just different model argument\n",
        "# Customize this with any other arguments/hyperparameters you might need\n",
        "def train_and_validate(model_type, device, lr, train_dataloader, val_dataloader, num_epochs):\n",
        "    \"\"\"\n",
        "    Train and validate your MLP or Conv1D models on sound event classification.\n",
        "\n",
        "    Parameters:\n",
        "        model_type (str): The type of model to train (e.g., 'mlp', 'conv1d').\n",
        "        device (str): The device to use for training ('cuda' or 'cpu').\n",
        "        lr (float): The learning rate for the optimizer.\n",
        "        train_dataloader (DataLoader): The dataloader for training data.\n",
        "        val_dataloader (DataLoader): The dataloader for validation data.\n",
        "        num_epochs (int): The number of epochs to train the model.\n",
        "    Returns:\n",
        "        Arrays of training and validation losses and accuracies.\n",
        "    \"\"\"\n",
        "    # Instantiate your model\n",
        "    # use .to(device) to move it to GPU if possible\n",
        "    # I've left the model params (hidden dim/input etc. out of the args to this fn, but add back if you want/need)\n",
        "    if model_type == \"mlp\":\n",
        "        model = None\n",
        "    elif model_type == \"conv1d\":\n",
        "        model = None\n",
        "\n",
        "    # Optimizer setup, on all of our model parameters\n",
        "    # Learning rate parameter here is something to play with, as well as optimizer type\n",
        "    opt = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    # Define the loss function, we'll use cross entropy here\n",
        "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    best_val_accuracy = 0.0\n",
        "    all_train_loss_epochs = [] # accumulate these for plots and monitoring\n",
        "    all_train_acc_epochs = []\n",
        "    all_val_loss_epochs = []\n",
        "    all_val_acc_epochs = []\n",
        "\n",
        "    # TODO : TRAINING LOOP\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # May need some things to track loss/accuracy per epoch here\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for inputs, targets in tqdm(train_dataloader, desc=f\"Train Epoch {epoch+1}/{num_epochs}\"):\n",
        "            # TODO: Move inputs and targets to GPU if available\n",
        "            # TODO: Zero gradients\n",
        "            # TODO: Forward pass\n",
        "            # TODO: Compute loss\n",
        "            # TODO: Backpropagation\n",
        "            # TODO: Update model parameters\n",
        "            # TODO: Compute training accuracy metrics (your model returns probabilities, so need to get labels\n",
        "            # TODO: Any other loss/acc accumulation/monitoring at epoch-level outside of this loop\n",
        "            pass\n",
        "\n",
        "        avg_train_loss = None\n",
        "        train_accuracy = None\n",
        "        all_train_loss_epochs.append(avg_train_loss)\n",
        "        all_train_acc_epochs.append(train_accuracy)\n",
        "\n",
        "        print(f\"Train Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
        "\n",
        "        # TODO : VALIDATION LOOP\n",
        "        model.eval()\n",
        "\n",
        "        for batch in tqdm(val_dataloader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
        "            # TODO: Move inputs and targets to GPU if available\n",
        "            # TODO: Forward pass\n",
        "            # TODO: Compute loss\n",
        "            # TODO: Compute validation accuracy metrics\n",
        "            pass\n",
        "\n",
        "        avg_val_loss = None\n",
        "        val_accuracy = None\n",
        "        print(f\"Validation Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\\n\")\n",
        "\n",
        "        # Save the best model so far based on validation accuracy\n",
        "        # This isn't always the last epoch, which is why we check per epoch\n",
        "        # You could also save based on lowest validation loss\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), f\"best_model_{model_type}.pth\")\n",
        "            print(f\"Best model updated with validation accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "        return (all_train_loss_epochs,\n",
        "                all_train_acc_epochs,\n",
        "                all_val_loss_epochs,\n",
        "                all_val_acc_epochs)\n",
        ""
      ],
      "metadata": {
        "id": "rQ9-axmeYd6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tdt_oDAylXFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 💯 Define evaluation methods 💯\n",
        "Now let's write some methods to take a saved, trained model and evaluate it on the test set. The validation set (monitored during training)gives us an idea of how the model will generalize to unseen data, so you should expect the validation performance to be similar to test."
      ],
      "metadata": {
        "id": "hJLTXKcsgmsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Complete this function to load the trained model\n",
        "def load_best_model(model_type, device):\n",
        "    \"\"\"\n",
        "    Load the best saved model from the file system.\n",
        "\n",
        "    Parameters:\n",
        "        model_type (str): The type of model to load ('mlp' or 'conv1d').\n",
        "        device (str): The device to load the model onto ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        model: The loaded model with trained weights.\n",
        "    \"\"\"\n",
        "    if model_type == \"mlp\":\n",
        "        model = None  # TODO : Instantiate your MLP model here\n",
        "    elif model_type == \"conv1d\":\n",
        "        model = None  # TODO ; Instantiate your Conv1D model here\n",
        "\n",
        "    model.load_state_dict(torch.load(f\"best_model_{model_type}.pth\", map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"Loaded best model: best_model_{model_type}.pth\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# TODO : Complete this method for evaluating your trained model\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def eval_model(model, test_dataloader, device):\n",
        "    \"\"\"\n",
        "    Test the loaded model on a test dataset.\n",
        "\n",
        "    Parameters:\n",
        "        model: The trained model to evaluate.\n",
        "        test_dataloader (DataLoader): The dataloader for test data.\n",
        "        device (str): The device to use for testing ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        results_dict (dict): Dictionary of average acc, precision, recall, and F1 score across all data points.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    # This should have a scal\n",
        "    results_dict = {'avg_acc': 0,\n",
        "                    'avg_precision': 0,\n",
        "                    'avg_recall': 0,\n",
        "                    'avg_f1': 0}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(test_dataloader, desc=\"Testing Model\"):\n",
        "            # TODO : get the model output and calculate the evaluation metrics\n",
        "            # Get accuracy, precision, recall, and F1 score (can use built-in sklearn methods)\n",
        "            # Accumulate targets and model output predictions so you can compute the overall avg metrics\n",
        "            pass\n",
        "\n",
        "    return results_dict\n",
        ""
      ],
      "metadata": {
        "id": "4xM-7ntzglya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "S1mg-mmKlYan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training your sound event classifiers 💪"
      ],
      "metadata": {
        "id": "aO36NQKJivN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - DATA : Initialize your training, validation, and test datasets and dataloaders\n",
        "# Note: log mel spec is a good starting point to use as your feature here, but feel free to explore others and compare results\n",
        "train_dataset = None\n",
        "train_dataloader = None\n",
        "\n",
        "val_dataset = None\n",
        "val_dataloader = None\n",
        "\n",
        "test_dataset = None\n",
        "test_dataloader = None\n",
        "\n",
        "# TODO : Use your method above to train your MLP model.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # see if you have GPU access\n",
        "print('Device: ', device)\n"
      ],
      "metadata": {
        "id": "0IH1Awtmi7cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train your MLP\n",
        "The training may take some time! On T-4 Colab GPU, my model was taking around 30 seconds for a training loop and 10 seconds for validation in one epoch. I recommend starting by training for less epochs first to make sure everything is working as you expected."
      ],
      "metadata": {
        "id": "_B659TIQZx7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Run this cell to train your MLP\n",
        "# Modify any params accordingly\n",
        "\n",
        "tr_loss_ep, tr_acc_ep, val_loss_ep, val_acc_ep = train_and_validate(model_type=\"mlp\",\n",
        "                                                                    device=device,\n",
        "                                                                    train_dataloader=train_dataloader,\n",
        "                                                                    val_dataloader=val_dataloader,\n",
        "                                                                    num_epochs=50,\n",
        "                                                                    lr=0.001) # I found LR=0.001 to be most successful for MLP, but will depend on your architecture\n",
        ""
      ],
      "metadata": {
        "id": "vPLISLNBZ1I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Plot your training and validation loss and accuracy curves\n",
        "# Plotting!"
      ],
      "metadata": {
        "id": "XXSx0ViAnlz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate your MLP\n",
        "Great, now that you've trained the model, let's evaluate it on unseen test data!"
      ],
      "metadata": {
        "id": "U0hgoCg5c5bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Run this cell to evaluate your MLP\n",
        "trained_model_mlp = load_best_model(\"mlp\", device)\n",
        "trained_model_mlp.eval()\n",
        "results_mlp = eval_model(trained_model_mlp, test_dataloader, device)"
      ],
      "metadata": {
        "id": "7XorM6Ijeipq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uCkPqbYOlZ5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train your Conv1D\n",
        "Do the same for your Conv1D model, using the functions above!"
      ],
      "metadata": {
        "id": "Dzm-8cRuYPA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Run this cell to train your Conv1D\n",
        "# Modify any params accordingly\n",
        "tr_loss_ep, tr_acc_ep, val_loss_ep, val_acc_ep = train_and_validate(model_type=\"conv1d\",\n",
        "                                                                    device=device,\n",
        "                                                                    train_dataloader=train_dataloader,\n",
        "                                                                    val_dataloader=val_dataloader,\n",
        "                                                                    num_epochs=50,\n",
        "                                                                    lr=0.1) # I found LR=0.1 to work well for conv1d here (but depends on your model)"
      ],
      "metadata": {
        "id": "TElMWg9Ojp-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Plot your training and validation loss and accuracy curves\n",
        "# Plotting!"
      ],
      "metadata": {
        "id": "TwJvjxysnx8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test your Conv1D"
      ],
      "metadata": {
        "id": "OCAbq9bYNrAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Run this cell to evaluate your Conv1D\n",
        "trained_model_conv = load_best_model(\"conv1d\", device)\n",
        "trained_model_conv.eval()\n",
        "results_conv = eval_model(trained_model_conv, test_dataloader, device)"
      ],
      "metadata": {
        "id": "EGtI_dNyj2I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jcqk2EHQlbSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⚠️ **TODO Written Questions:** ⚠️\n",
        "Great job! Now that you've trained your models, let's reflect a bit on them:\n",
        "4. How does the performance of your two models compare? Explain why you think the models performed as they did.\n",
        "5. What do you observe from the train/validation loss/accuracy plots? Why do you think this is happening?\n",
        "5. There are a handful of components of model training and architectures that we discussed in class/in the slides, that we didn't experiment with here. What is one other factor you could incorporate into your data processing, model architecture or training pipeline to analyze its affect on model performance? (You don't actually have to implement it)\n"
      ],
      "metadata": {
        "id": "FrzNYDAkkNlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "skpcF750lQVd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv5B7XLFmFtE"
      },
      "source": [
        "# Part 2: Working with **time**... voice *detection* with neural networks [4 pts]\n",
        "Above, we were working with **clip**-level output, e.g. a single label sound class given an entire input audio clip. With audio (vs. images for example), **time** plays a big role - in real-world audio such as music or people speaking, many different sounds are happening in a single audio clip at different points in time. Oftentimes assigning a single sound class to an entire-clip isn't enough. That's where **frame**-level outputs come into play, and also where neural networks can shine by taking temporal structure into account.\n",
        "\n",
        "Here we will be transitioning to sound event ***detection***: which is detecting what and importantly ***when*** a sound is occuring. Your model input is still the same audio clip or spectrogram as before (but a different dataset), but now the model will output a sequence, such as a class prediction per time-frame.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**EnvSpeech** is a dataset I created of 5-second, 44.1khz audio clips containing background audio (clips from ESC-50 categories such as rain) and a single speech clip from Librispeech. The speech segments are between 0.5 and 5 seconds long, and can begin anywhere in the clip. **Each soundscape file has an accompanying label file**, which is a 5-dimensional binary vector representing the presence or not of speech for each second (i.e. \"bin\") of the audio clip. For example, if the speech clip in the soundscape is from 1.1-1.7 seconds in the clip, the label will be [0,1,0,0,0]. The speech can span multiple frames, e.g. if the speech clip is from 1.5-4.5 seconds, the label would be [0,1,1,1,1]. This task is known as \"voice activity detection\" or \"VAD\".\n",
        "\n",
        "---\n",
        "⚠️ **In this section, things will be a bit different than Part 1: I'll provide the model code, and you'll answer some questions about the design decisions.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Data Downloading\n",
        "1. Download the zip of the EnvSpeech dataset from Google drive here: https://drive.google.com/file/d/14xcUpv-bd40D4-1n-k3KtpVdBMJOmpid/view?usp=sharing.\n",
        "2. Upload it to *your* Google Drive, so you can access it here via mounting.\n",
        "3. Unzip inside the Colab virtual machine (MUCH faster than loading individual files from drive)\n",
        "    - ex: `!unzip \"/content/drive/MyDrive/EnvSpeechData.zip\" -d \"/content\"`\n",
        "4. Use the code below to mount your Google drive to this Colab notebook.\n",
        "5. Now you can access the data here - but note you'll have to unzip agin if the Colab notebook kernel restarts/goes down.\n"
      ],
      "metadata": {
        "id": "lyqpu_GXof_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/EnvSpeechData.zip\" -d \"/content\""
      ],
      "metadata": {
        "id": "5ehLaiobo_Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RsxZ0agKpAo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtMqljPM7T3F"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFldQ7m1iK2e"
      },
      "source": [
        "### 🔎 But first: check out the data 🔎\n",
        "This is always a good step to make sure the input and target for your model are exactly what you are expecting.\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "1. Load an audio file from EnvSpeech (in `audio` directory`\n",
        "2. Plot the waveform\n",
        "3. Plot the log mel spectrogram\n",
        "4. Play the audio\n",
        "5. Load the associated label (in `labels_5`) and print it\n",
        "6. Load the full metadata csv (in `env_speech_meta.csv`), locate the sample you're examining, and confirm that your label and timestamps match what you're expecting, and what you see in the spectrogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxGEPbm2iJNq"
      },
      "outputs": [],
      "source": [
        "# TODO : the 6 points above exploring your data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⌛ Define your dataloader for EnvSpeech ⌛\n"
      ],
      "metadata": {
        "id": "h-4iYPpipsnZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSxLdvot7ZAH"
      },
      "outputs": [],
      "source": [
        "# TODO : design your EnvSpeech dataloader, filling in the blanks and building off of the template in Part 0\n",
        "\n",
        "class EnvSpeechDataset(Dataset):\n",
        "    def __init__(self, data_dir, data_split, spec_type=\"log_mel\", sr=44100, n_fft=1024, hop_length=512, n_mels=128):\n",
        "        self.data_dir = data_dir\n",
        "        dataframe = pd.read_csv(os.path.join(data_dir, 'env_speech_meta.csv'))\n",
        "\n",
        "        ## TODO : Based on data_split arg, filter the dataframe\n",
        "        # Metadata CSV has a column `split` that you can use to filter\n",
        "        filtered_df = None\n",
        "\n",
        "        # TODO : get list of file IDs in this split\n",
        "        # You'll use this for loading the audio/labels later\n",
        "        self.file_ids = None\n",
        "\n",
        "        self.spec_type = spec_type\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.n_mels = n_mels\n",
        "        self.sr = sr\n",
        "\n",
        "        print(f'Num files in {data_split}: {len(self.file_ids)}')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO : Given a file ID eg. 'soundscape1234'\n",
        "        # (1) get the label and audio filepaths\n",
        "        # (2) load your wavefrom at the specified sample rate\n",
        "        # (3) load the associated numpy label\n",
        "        # (4) given a spectrogram argument self.spec_type, compute the appropriate feature (lin_pwr, log, log_mel)\n",
        "        # (5) uncomment the spectrogram normalization line\n",
        "        # (6) return the spectrogram and speech activity labels (an array)\n",
        "        audio_path = os.path.join(self.data_dir, f'audio/{self.file_ids[idx]}.wav')\n",
        "        label_path = os.path.join(self.data_dir, f'labels/{self.file_ids[idx]}.npy')\n",
        "\n",
        "        label = None\n",
        "        spectrogram = None # use the params from __init__\n",
        "\n",
        "        # TODO : uncomment this line for spectrogram standardization\n",
        "        # You could also try per-sample min-max normalization like we did in assign. 2!\n",
        "        # spectrogram = (spectrogram - spectrogram.mean()) / (spectrogram.std() + 1e-6)\n",
        "        return spectrogram, label"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔎 Test out your dataloader before we get into the model code 🔎"
      ],
      "metadata": {
        "id": "JsDu_vEorhkg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQEKYI6AAKzF"
      },
      "outputs": [],
      "source": [
        "# TODO : Instantiate your EnvSpeech class and dataloader\n",
        "dataset = None\n",
        "dataloader = None\n",
        "\n",
        "# TODO : Iterate over the dataloader and print the shapes of your spectrogram and label batch for one batch\n",
        "print('my dataloader shapes')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "c_tc9wjsrrnQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOSL3xXhS7pK"
      },
      "source": [
        "## 🤖 Vocal activity detection model design 🤖\n",
        "Now, we'll be using Conv-2D for frame-wise voice activity detection to take advantage of the full spatio-temporal benefits that convolutional layers have to offer. For this model:\n",
        "- **Model input**: a log-mel spectrogram and it's associated voice detection array, a binary vectory signaling detection in a given frame (second of audio) if 1, else 0. Shape: `(batch, 128, 431)` and `(batch, 5)`\n",
        "- **Model output**: a vector of probabilities over the number of frames (\"classes\"). Shape:  `(batch, 5)`\n",
        "\n",
        "⚠️ **TODO:** Below, I've defined the class `CNNFrameClassifier`. Step through the code, which includes 3 numbered TODO questions about the model, and answer them in the cell after the model defintion.\n",
        "\n",
        "Note that a couple of concepts that weren't explicitly mentioned in class are in the model definition for performance purposes, which gives an opportunity to do some extra research (e.g. adaptive avg. pooling) - many exciting things to learn!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_2b7CYmKxos"
      },
      "outputs": [],
      "source": [
        "# TODO: Step through this code and answer the 3 questions below about the architecture\n",
        "class CNNFrameClassifier(nn.Module):\n",
        "    def __init__(self, input_channels=1, num_out_frames=5):  # 1 \"class\" per second\n",
        "        \"\"\"\n",
        "        A Convolutional Neural Network (CNN) for frame-wise classification.\n",
        "\n",
        "        Args:\n",
        "            input_channels (int): Number of input channels (e.g., 1 for spectrograms).\n",
        "            num_out_frames (int): Number of time steps in the output (e.g., per-second predictions).\n",
        "        \"\"\"\n",
        "        super(CNNFrameClassifier, self).__init__()\n",
        "\n",
        "        # First convolutional layer: Apply a 3x3 convolution to extract basic features\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=input_channels,\n",
        "            out_channels=16,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1  # Padding ensures the spatial dimensions remain the same\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(16)  # Normalization for stability\n",
        "\n",
        "        # Second convolutional layer: Extract deeper feature representations\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Third convolutional layer: Further increase feature complexity\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # **TODO [1]** Explain why we use a Conv2D layer here instead of a fully connected layer.**\n",
        "        self.conv_out = nn.Conv2d(64, 1, kernel_size=(32, 1))\n",
        "\n",
        "        # Adaptive pooling ensures that we get exactly `num_out_frames` time steps in the output\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, num_out_frames))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the CNN.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (batch, channels, frequency, time).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Predictions of shape (batch, num_out_frames, 1).\n",
        "        \"\"\"\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)))  # Apply first convolution + activation\n",
        "        x = F.max_pool2d(x, (2, 2))  # Downsample the feature map\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))  # Apply second convolution + activation\n",
        "        x = F.max_pool2d(x, (2, 2))  # Further downsample\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))  # Third convolution + activation\n",
        "\n",
        "        # **TODO [2]** What does this layer do to the feature map?**\n",
        "        x = self.conv_out(x)  # Collapse frequency dimension\n",
        "\n",
        "        # **TODO [3]** What is the role of adaptive pooling here?**\n",
        "        x = self.adaptive_pool(x)  # Ensure fixed output time steps\n",
        "\n",
        "        # Reshape the output to match the expected format\n",
        "        x = x.squeeze(2).permute(0, 2, 1)  # Reshape to (batch, num_out_frames, 1)\n",
        "\n",
        "        return x  # Predictions per time step\n",
        "\n",
        "# Running this may also help visualiaze the model architecture flow\n",
        "vad_model = CNNFrameClassifier(num_out_frames=10)\n",
        "summary(vad_model, input_size=(4,1, 128, 431))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ **Responses to questions in architecture above:**\n",
        "1.\n",
        "2.\n",
        "3."
      ],
      "metadata": {
        "id": "ryYoWhvZxIl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZODr6mysyoeJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs0vTGK0PrE_"
      },
      "source": [
        "## Training your voice activity detection model 💪\n",
        "Below, set up your model training and validation script using the model defined above. A few tips for this task:\n",
        "\n",
        "- We use `torch.nn.BCEWithLogitsLoss` for the loss here, since we are framing this as a multi-label classification.\n",
        "- Your model is designed to output a 5-dimensional logit (probability) vector. To get the validation *accuracy*, you will need to apply a final activation function that allows for multi-labels (because you can have active speech at more than 1 index in your label, e.g. [1,1,0,0,0]).\n",
        "- Because this is a multi-label problem, the accuracy metrics are a bit trickier. For simplicity, here we'll work with these two metrics:\n",
        "    - **\"per-sample\" accuracy**: for every item in the predicted array (of dimension 5), measure if this binary prediction is correct. This treats each index individually. The **random baseline** here is 50%, as it is essentially an individual binary classification problem.\n",
        "    - **\"exact-match\" accuracy**: if every index in the predicted output is correct, this = 1, else 0. For example, a prediction of [1,1,0,0,0] for a target [1,1,0,0,0] would be correct, but anything else would be incorrect. This is a more difficult metric. The **random baseline** here is 0.5^5 = 0.03125 = 3.125%, measuring the independent chance of each frame label being correct\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : complete the training script below - very similar to part 1!\n",
        "def train_and_validate_voice_detection(model, device, train_dataloader, val_dataloader, num_epochs, lr=0.001):\n",
        "    \"\"\"\n",
        "    Train and validate your voice detection CNN.\n",
        "\n",
        "    Parameters:\n",
        "        model : Instantiation of CNNFrameClassifier model.\n",
        "        device (str): The device to use for training ('cuda' or 'cpu').\n",
        "        lr (float): The learning rate for the optimizer.\n",
        "        train_dataloader (DataLoader): The dataloader for training data.\n",
        "        val_dataloader (DataLoader): The dataloader for validation data.\n",
        "        num_epochs (int): The number of epochs to train the model.\n",
        "    Returns:\n",
        "        Arrays of training and validation losses and accuracies.\n",
        "    \"\"\"\n",
        "    # Model to GPU\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimizer setup\n",
        "    opt = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    # Define the loss function\n",
        "    loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "    best_val_accuracy = 0.0\n",
        "    all_train_loss_epochs = []\n",
        "    all_train_acc_epochs = []\n",
        "    all_val_loss_epochs = []\n",
        "    all_val_acc_epochs = []\n",
        "\n",
        "    # TODO : TRAINING LOOP\n",
        "    for epoch in range(num_epochs):\n",
        "        total_train_loss = 0.0\n",
        "        total_correct_train = 0\n",
        "        total_exact_matches_train = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for inputs, targets in tqdm(train_dataloader, desc=f\"Train Epoch {epoch+1}/{num_epochs}\"):\n",
        "            # TODO: Move inputs and targets to GPU if available\n",
        "            # TODO: Zero gradients\n",
        "            # TODO: Forward pass\n",
        "            # TODO: Compute loss\n",
        "            # TODO: Backpropagation\n",
        "            # TODO: Update model parameters\n",
        "            # TODO: Compute training accuracy metrics (your model returns probabilities, so need to get labels...choose activation wisely\n",
        "            # TODO: Any other loss/acc accumulation/monitoring at epoch-level outside of this loop\n",
        "            pass\n",
        "\n",
        "        avg_train_loss = None\n",
        "        train_per_label_acc = None\n",
        "        train_exact_match_acc = None\n",
        "\n",
        "        all_train_loss_epochs.append(avg_train_loss)\n",
        "        all_train_acc_epochs.append(train_exact_match_acc)\n",
        "\n",
        "        print(f\"Train Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Per-Label Accuracy: {train_per_label_acc:.4f}, Exact Match Accuracy: {train_exact_match_acc:.4f}\")\n",
        "\n",
        "        # TODO : VALIDATION LOOP\n",
        "        total_val_loss = 0.0\n",
        "        total_correct_val = 0\n",
        "        total_exact_matches_val = 0\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in tqdm(val_dataloader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
        "                # TODO: Move inputs and targets to GPU if available\n",
        "                # TODO: Forward pass\n",
        "                # TODO: Compute loss\n",
        "                # TODO: Compute validation accuracy metrics\n",
        "                pass\n",
        "\n",
        "        avg_val_loss = None\n",
        "        val_per_label_acc = None\n",
        "        val_exact_match_acc = None\n",
        "\n",
        "        all_val_loss_epochs.append(avg_val_loss)\n",
        "        all_val_acc_epochs.append(val_exact_match_acc) # use exact match here\n",
        "\n",
        "        print(f\"Validation Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}, Per-Label Accuracy: {val_per_label_acc:.4f}, Exact Match Accuracy: {val_exact_match_acc:.4f}\\n\")\n",
        "\n",
        "        # TODO: Save the best model based on validation accuracy - exact match\n",
        "        if val_exact_match_acc > best_val_accuracy:\n",
        "            best_val_accuracy = val_exact_match_acc\n",
        "            torch.save(model.state_dict(), \"best_model_voice_detection.pth\")\n",
        "            print(f\"Best model updated with validation accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "    return all_train_loss_epochs, all_train_acc_epochs, all_val_loss_epochs, all_val_acc_epochs\n"
      ],
      "metadata": {
        "id": "uiV11Mloy-Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up dataloadewrs\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print('Device: ', device)\n",
        "\n",
        "train_dataset = None\n",
        "train_dataloader = None\n",
        "\n",
        "val_dataset = None\n",
        "val_dataloader = None\n",
        "\n",
        "test_dataset = None\n",
        "test_dataloader = None"
      ],
      "metadata": {
        "id": "yfN98FZJ3hgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Run this cell to train your frame detector\n",
        "# Sanity check : is your model doing better than random performance?\n",
        "model = CNNFrameClassifier(input_channels=1, num_out_frames=5)\n",
        "tr_loss_ep,tr_acc_ep, val_loss_ep, val_acc_ep = train_and_validate_voice_detection(model=model,\n",
        "                                                                    device=device,\n",
        "                                                                    train_dataloader=train_dataloader,\n",
        "                                                                    val_dataloader=val_dataloader,\n",
        "                                                                    num_epochs=50,\n",
        "                                                                    lr=0.001) # I found LR=0.001 to be most successful here, but something to play with\n"
      ],
      "metadata": {
        "id": "YX9DPxbO3HeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsfM_g94hEBv"
      },
      "outputs": [],
      "source": [
        "# TODO : Plot your training and validation loss and accuracy curves\n",
        "# Plotting!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BVK7EA6-9xpY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv8nG_pQWBTJ"
      },
      "source": [
        "## 💯 Evaluation 💯\n",
        "We'll quickly make some new model loading and eval functions for this task."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Complete this function to load the trained model\n",
        "def load_best_model_voice_detection(device):\n",
        "    \"\"\"\n",
        "    Load the best saved model from the file system.\n",
        "\n",
        "    Parameters:\n",
        "        device (str): The device to load the model onto ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        model: The loaded model with trained weights.\n",
        "    \"\"\"\n",
        "    model.load_state_dict(torch.load(f\"best_model_voice_detection.pth\", map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"Loaded best model: best_model_detection.pth\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# TODO : Complete this method for evaluating your trained model\n",
        "def eval_model_voice_detection(model, test_dataloader, device):\n",
        "    \"\"\"\n",
        "    Test the loaded model on a test dataset.\n",
        "\n",
        "    Parameters:\n",
        "        model: The trained model to evaluate.\n",
        "        test_dataloader (DataLoader): The dataloader for test data.\n",
        "        device (str): The device to use for testing ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        results_dict (dict): Dictionary of average per-sample and exact match accuracies.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # This should have a scal\n",
        "    results_dict = {'avg_per_sample_acc': 0,\n",
        "                    'avg_exact_match_acc': 0}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(test_dataloader, desc=\"Testing Model\"):\n",
        "            # TODO : get the model output and calculate the evaluation metrics\n",
        "            pass\n",
        "\n",
        "    return results_dict\n"
      ],
      "metadata": {
        "id": "PYv5Cn1x4AWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Run this cell to evaluate your CNN on the test dataset\n",
        "trained_model_cnn = load_best_model_voice_detection(device)\n",
        "trained_model_cnn.eval()\n",
        "results_cnn = eval_model(trained_model_cnn, test_dataloader, device)"
      ],
      "metadata": {
        "id": "f2VEe11x4sdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Hd-CVnmG9tl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚠️ **TODO Written Questions - Voice Detection:** ⚠️\n",
        "1. Explain which activation function you used in the voice detection model at evaluation time to get the final output labels. Is this different from what was used in Part 1 for sound event classification?\n",
        "2. Here we used a Conv2D for this task that has frame-wise output. Is there a way we could use an MLP to get frame-wise predictions?\n"
      ],
      "metadata": {
        "id": "wE4_LsaY5F72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "f-p_TPnh9ubm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus (1pt): data augmentation\n",
        "For either parts of the lab (sound classification or voice detection), experiment with **data augmentation** during training. This will require enhancing your dataloader and providing an option to augment the data in some of the ways we discussed in class. Update the dataloader and re-train+evaluate one of the models using data augmentation. Report here what you updated and the affect you saw on model performance."
      ],
      "metadata": {
        "id": "N9Lfqewr5u5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Oeku-Gy399xf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjMcT-Cr4kKp"
      },
      "source": [
        "Code authored by Julia Wilkins, with some points adapted from [Deep Learning](https://github.com/interactiveaudiolab/course-deep-learning/blob/main/notebooks/notebook_2_nn.ipynb) at Northwestern University with Bryan Pardo."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SZDpKa3NlUR9",
        "AuK7dA9Vi4tW",
        "mv5B7XLFmFtE"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}